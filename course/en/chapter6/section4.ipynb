{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Cio_qjucTZ"
      },
      "source": [
        "# Normalization and pre-tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hdXJmjvucTm"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FPIHt6xucTp"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization and pre-tokenization"
      ],
      "metadata": {
        "id": "e4eAJbBGujl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you‚Äôre familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n",
        "\n",
        "The ü§ó Transformers tokenizer has an attribute called backend_tokenizer that provides access to the underlying tokenizer from the ü§ó Tokenizers library:"
      ],
      "metadata": {
        "id": "1L4VPwLUuuLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "1e22804cb6184cab9ae8b61190bf9c50",
            "ff628da1b83b4fd9a6895281aeb08492",
            "74faf0622dcd49778d3086f94c720c6d",
            "5a4f8afdf7874586bbc97a50611b4561",
            "71f4e67daf034b6396cbebdb8383c231",
            "9124ccf95f3d45649e50a8e93d5256ec",
            "1002b3fb3ef345ff9608f9fb22654b14",
            "21cd56a42b394336a835eee94c1d21b4",
            "ab9d9a6698364a66816209dcc089c023",
            "c706018381814e6a957bff2f7d81698e",
            "8861c541ef36436b9201e32ee2b11eb5",
            "4e82169f6fc04291b06b544d65d5d0d4",
            "54459e3f702f4407ad2e0ca267b12b8a",
            "c5a89f4535e7434ab0c7445d04356d26",
            "04e9d9dc80fc4a3ebbdc03481f4e6916",
            "c585328498aa43cfb47570691b41f6d1",
            "31126205eb0e4677bc3ba96e10839dd8",
            "cb8c946943534739ad609aacec53e4fd",
            "0e046f0c7dab4e9bbf86745b49e12966",
            "86dc837e92bf4fbd8853412fcba5fd91",
            "335c5f59d0054751bb7dcda88e8e60e7",
            "33bb39e55f4149a6821d5e7413ec3176",
            "6d167f285b8341ecb613d8447159f2a4",
            "4c6ead086d4e4d388b4dfa98e3fbb083",
            "0ba1401bf76c41878354d31d09f26c07",
            "da03cc8604c64f5daf8325e98eac75da",
            "2dba4253bb2346e8ac5a656a30104cdf",
            "a9b43d79566d4546975859a58f16c31f",
            "2e3ca44e9012443683d4b80cfcc86944",
            "d1e9981c30454bbb86898c7bba74a98a",
            "84076ae0e5f6486ebdfc7b0f371561c7",
            "5cb6d7dff24842b8b15ffd3e1695a812",
            "a7ff89f24dc24c0891aa1265e451b3c9",
            "562a7b7ac2684e2dac783949a3800639",
            "f0af866821e243778271bf8e62320dc5",
            "3eb6cf24be6e4f29a57634d433455612",
            "08635139ba1747ff9c0b222546e0be8c",
            "f2e5931e563b457bac551fbcd01e7c1b",
            "889c3f40b75843e6b4f9e2eada7db4e0",
            "381421382f0c4eea9a7a0a95793fc310",
            "13327ce89f3d49b1a0817d75e495a37c",
            "8fce5874d2a54782ad21933b2abe6634",
            "080e984af28b4e249b572fa0ff4f9b73",
            "d03c2bbadbe246b3a76ae8fb593d83c5"
          ]
        },
        "id": "BHPz42vOucTt",
        "outputId": "d337107c-3942-46a0-e400-c4107716fe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e22804cb6184cab9ae8b61190bf9c50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e82169f6fc04291b06b544d65d5d0d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d167f285b8341ecb613d8447159f2a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562a7b7ac2684e2dac783949a3800639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tokenizers.Tokenizer'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(type(tokenizer.backend_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalizer attribute of the tokenizer object has a normalize_str() method that we can use to see how the normalization is performed:"
      ],
      "metadata": {
        "id": "SdmWKyDkvX5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFBIMcmNucTy",
        "outputId": "5b0d2198-ca44-45f6-cd3c-83d0871dff3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are u?\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, since we picked the bert-base-uncased checkpoint, the normalization applied lowercasing and removed the accents."
      ],
      "metadata": {
        "id": "YxoUl6xrvhCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-tokenization"
      ],
      "metadata": {
        "id": "uArCNb6OvrFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "enxsjs17vtrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVFynfm8ucT6",
        "outputId": "8bd57f02-26ac-49a1-a6b2-24ad6bc89ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQtgAIaoucT8",
        "outputId": "1e4620b4-5d11-48fa-8e83-c71a787fe3fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),\n",
              " ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBLJprnUucT-",
        "outputId": "ff7137ae-75d6-4b15-e938-b7a20c9da0ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Normalization and pre-tokenization",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
